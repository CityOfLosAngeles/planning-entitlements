{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New merges for master_pcts\n",
    "* Losing appeals cases because of an inner join\n",
    "* These appeals child cases aren't associated with an AIN, but we can grab it from parent case\n",
    "* But, merging using PARENT_CASE rather than CASE_ID means it's a m:m merge (gross!)\n",
    "* Figure out if we can expand the merges, switch the order, and create new master_pcts\n",
    "* Add print statements to check the length of the df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import intake\n",
    "\n",
    "catalog = intake.open_catalog(\"../catalogs/*.yml\")\n",
    "bucket_name = 'city-planning-entitlements'\n",
    "\n",
    "# Import data\n",
    "cases = pd.read_parquet(f's3://{bucket_name}/data/raw/tCASE.parquet')\n",
    "app = pd.read_parquet(f's3://{bucket_name}/data/raw/tAPLC.parquet')\n",
    "geo_info = pd.read_parquet(f's3://{bucket_name}/data/raw/tPROP_GEO_INFO.parquet')\n",
    "la_prop = pd.read_parquet(f's3://{bucket_name}/data/raw/tLA_PROP.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subset dataframes before merging\n",
    "keep_col = ['CASE_ID', 'APLC_ID', 'CASE_NBR', \n",
    "                'CASE_SEQ_NBR', 'CASE_YR_NBR', 'CASE_ACTION_ID', \n",
    "                'CASE_FILE_RCV_DT', 'CASE_FILE_DATE', 'PARNT_CASE_ID']\n",
    "\n",
    "cases1 = (cases.assign(\n",
    "    # Grab the year-month from received date\n",
    "    CASE_FILE_DATE = pd.to_datetime(cases['CASE_FILE_RCV_DT']).dt.to_period('M'),\n",
    ")[keep_col])\n",
    "\n",
    "app1 = app[['APLC_ID', 'PROJ_DESC_TXT']]\n",
    "geo_info1 = geo_info[['CASE_ID', 'PROP_ID']].drop_duplicates()\n",
    "la_prop1 = la_prop[la_prop.ASSR_PRCL_NBR.notna()][['PROP_ID', 'ASSR_PRCL_NBR']]\n",
    "\n",
    "# Identify parent cases\n",
    "cases1['parent_is_null'] = cases1.PARNT_CASE_ID.isna()\n",
    "cases1['PARENT_CASE'] = cases1.apply(lambda row: row.CASE_ID if row.parent_is_null == True \n",
    "                                         else row.PARNT_CASE_ID, axis = 1)\n",
    "\n",
    "# Keep cases from 2010 onward\n",
    "cases2 = cases1[cases1.CASE_FILE_DATE.dt.year >= 2010]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First merge is between cases and geo_info\n",
    "We add on PROP_ID column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "m1 = pd.merge(cases2, geo_info1, on = 'CASE_ID', how = 'left', validate = '1:m')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_joins = m1[m1.PROP_ID.notna()]\n",
    "incorrect_joins = m1[m1.PROP_ID.isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# obs when we join cases and geo_info: 557696\n",
      "# obs where PROP_ID was NaN: 5781\n",
      "% where PROP_ID was NaN: 0.010365862405324765\n"
     ]
    }
   ],
   "source": [
    "print(f\"# obs when we join cases and geo_info: {len(m1)}\")\n",
    "print(f\"# obs where PROP_ID was NaN: {len(incorrect_joins)}\")\n",
    "print(f\"% where PROP_ID was NaN: {len(incorrect_joins) / len(m1)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# unique parents that were correctly joined, but also appear in incorrect_joins\n",
      "1709\n",
      "# unique parents in incorrect_joins: 5471\n"
     ]
    }
   ],
   "source": [
    "# Of these incorrect joins, do they share parent cases with ones that were joined?\n",
    "print(\"# unique parents that were correctly joined, but also appear in incorrect_joins\")\n",
    "print(f\"{incorrect_joins[incorrect_joins.PARENT_CASE.isin(correct_joins.PARENT_CASE)].PARENT_CASE.nunique()}\")\n",
    "print(f\"# unique parents in incorrect_joins: {incorrect_joins.PARENT_CASE.nunique()}\")\n",
    "\n",
    "# This shows a lot of parent cases won't get joined to a PROP_ID and AIN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a big rabbit hole that will never get rid of the m:m merge.\n",
    "\n",
    "1. There are PARENT_CASES where some of their CASE_IDs have PROP_ID merged, and some CASE_IDs that did not correctly join with PROP_ID. These PARENT_IDs will have some obs in correctly_joined and some in incorrectly_joined. This will involve m:m merge.\n",
    "1. There are also that fall completely within incorrectly_joined, and using PARENT_CASE, we can get PROP_IDs. This will involve m:m merge.\n",
    "1. There are also PARENT_CASEs that are only in incorrectly_joined, but we cannot get PROP_ID for them at the end of all this.\n",
    "\n",
    "Since the m:m cannot be averted, let's make it only slightly less painful by linking PROP_ID with AIN by itself, and getting rid of AINs not found in our crosswalk."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second merge is between geo_info and la_prop\n",
    "* To fix these incorrect joins, we would have to have allowed a m:m merge.\n",
    "* So, let's see if we can circumvent PROP_ID by mapping it to AIN directly.\n",
    "* But first, we force it to be a 1:m merge, and only keep unique PROP_IDs.\n",
    "* Then, bring in our crosswalk_parcels_tracts to make sure we only keep parcels we have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PROP_ID</th>\n",
       "      <th>AIN</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>34237.0</td>\n",
       "      <td>5160003902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>34306.0</td>\n",
       "      <td>5076007033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>34323.0</td>\n",
       "      <td>5407005016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>34169.0</td>\n",
       "      <td>5407002023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>33937.0</td>\n",
       "      <td>5124001012</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PROP_ID         AIN\n",
       "0  34237.0  5160003902\n",
       "1  34306.0  5076007033\n",
       "2  34323.0  5407005016\n",
       "3  34169.0  5407002023\n",
       "4  33937.0  5124001012"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Do a second merge for PROP_ID and AINs\n",
    "m2 = (pd.merge(geo_info1[[\"PROP_ID\"]].drop_duplicates(), \n",
    "               la_prop1, \n",
    "               on = \"PROP_ID\", how = \"left\", validate = \"1:m\")\n",
    "      .rename(columns = {\"ASSR_PRCL_NBR\": \"AIN\"})\n",
    "     )\n",
    "\n",
    "m2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "crosswalk_parcels_tracts = (pd.read_parquet(\n",
    "    \"s3://city-planning-entitlements/data/crosswalk_parcels_tracts_lacity.parquet\")\n",
    "    [[\"uuid\", \"AIN\"]]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# obs in m2: 314716\n",
      "# obs in m2 after dropping AINs not in our crosswalk: 283487\n"
     ]
    }
   ],
   "source": [
    "print(f\"# obs in m2: {len(m2)}\")\n",
    "m2 = m2[m2.AIN.isin(crosswalk_parcels_tracts.AIN)]\n",
    "print(f\"# obs in m2 after dropping AINs not in our crosswalk: {len(m2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Third merge is to fix our incorrectly joined df with a m:m merge\n",
    "\n",
    "* Combine the results of the previous 2 merges. Use PROP_ID to join cases with la_prop.\n",
    "* Leave the correctly_joined df alone.\n",
    "* Only do this m:m merge on the incorrectly_joined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# obs in incorrect_joins before m:m merge: 5781\n",
      "# unqiue PARENT_CASEs in incorrect_joins before m:m merge: 5471\n",
      "# obs in incorrect_joins after m:m merge: 19809\n",
      "# unqiue PARENT_CASEs in incorrect_joins after m:m merge: 5471\n"
     ]
    }
   ],
   "source": [
    "incorrect_joins_with_propid = pd.merge(incorrect_joins.drop(columns = [\"PROP_ID\"]), \n",
    "                                       geo_info1.rename(columns = {\"CASE_ID\": \"PARENT_CASE\"}), \n",
    "                                       on = \"PARENT_CASE\", how = \"left\", validate = \"m:m\")\n",
    "\n",
    "print(f\"# obs in incorrect_joins before m:m merge: {len(incorrect_joins)}\")\n",
    "print(f\"# unqiue PARENT_CASEs in incorrect_joins before m:m merge: {incorrect_joins.PARENT_CASE.nunique()}\")\n",
    "print(f\"# obs in incorrect_joins after m:m merge: {len(incorrect_joins_with_propid)}\")\n",
    "print(f\"# unqiue PARENT_CASEs in incorrect_joins after m:m merge: {incorrect_joins_with_propid.PARENT_CASE.nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# obs in incorrect_joins once we add in AIN: 19809\n",
      "# unqiue PARENT_CASEs once we add in AIN: 5471\n"
     ]
    }
   ],
   "source": [
    "incorrect_joins_with_ain = pd.merge(incorrect_joins_with_propid, m2,\n",
    "                                    on = \"PROP_ID\", how = \"left\", validate = \"m:1\"\n",
    "                                   )\n",
    "\n",
    "print(f\"# obs in incorrect_joins once we add in AIN: {len(incorrect_joins_with_ain)}\")\n",
    "print(f\"# unqiue PARENT_CASEs once we add in AIN: {incorrect_joins_with_ain.PARENT_CASE.nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# unique lost PARENT_CASEs: 3400\n",
      "Double check, try to find some in geo_info: 0\n"
     ]
    }
   ],
   "source": [
    "lost_parents = (incorrect_joins_with_ain[incorrect_joins_with_ain.PROP_ID.isna()]\n",
    "                [[\"PARENT_CASE\"]].drop_duplicates()\n",
    "               )\n",
    "\n",
    "print(f\"# unique lost PARENT_CASEs: {len(lost_parents)}\")\n",
    "print(f\"Double check, try to find some in geo_info: {len(geo_info1[geo_info1.CASE_ID.isin(lost_parents.PARENT_CASE)])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# obs in incorrect_joins that were fixed: 16307\n",
      "# unique PARENT_CASEs in incorrect_joins that were fixed: 2071\n"
     ]
    }
   ],
   "source": [
    "# Get rid of obs where we can't link to PROP_ID and AIN\n",
    "incorrect_joins_now_fixed = incorrect_joins_with_ain[incorrect_joins_with_ain.PROP_ID.notna()]\n",
    "print(f\"# obs in incorrect_joins that were fixed: {len(incorrect_joins_now_fixed)}\")\n",
    "print(f\"# unique PARENT_CASEs in incorrect_joins that were fixed: {incorrect_joins_now_fixed.PARENT_CASE.nunique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fourth merge is to merge correctly_joined with AIN info\n",
    "* Once done, can concatenate the correct / incorrect joins together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_joins_with_ain = pd.merge(correct_joins, m2, on = \"PROP_ID\", how = \"inner\", validate = \"m:1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate the 2 parts together\n",
    "m3 = (pd.concat([\n",
    "        correct_joins_with_ain, \n",
    "        incorrect_joins_now_fixed\n",
    "    ], axis=0)\n",
    "      .sort_values([\"CASE_ID\", \"AIN\", \"PROP_ID\"])\n",
    "      .drop_duplicates(subset = [\"CASE_ID\", \"AIN\"])\n",
    "      .reset_index(drop=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# obs in m3: 349897\n",
      "# obs in m1: 557696\n",
      "# unique CASE_IDs in m3: 49844\n",
      "# unique CASE_IDs in m1: 54944\n",
      "# unique PARENT_CASEs in m3: 47046\n",
      "# unique PARENT_CASEs in m1: 51924\n"
     ]
    }
   ],
   "source": [
    "print(f\"# obs in m3: {len(m3)}\")\n",
    "print(f\"# obs in m1: {len(m1)}\")\n",
    "\n",
    "print(f\"# unique CASE_IDs in m3: {m3.CASE_ID.nunique()}\")\n",
    "print(f\"# unique CASE_IDs in m1: {m1.CASE_ID.nunique()}\")\n",
    "\n",
    "print(f\"# unique PARENT_CASEs in m3: {m3.PARENT_CASE.nunique()}\")\n",
    "print(f\"# unique PARENT_CASEs in m1: {m1.PARENT_CASE.nunique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fifth merge is to add on project description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "m4 = pd.merge(m3, app1, on = \"APLC_ID\", how = \"left\", validate = \"m:1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# obs in m4: 349897\n",
      "# unique CASE_ID in m4: 49844\n",
      "# unique PARENT_CASEs in m4: 47046\n"
     ]
    }
   ],
   "source": [
    "print(f\"# obs in m4: {len(m4)}\")\n",
    "print(f\"# unique CASE_ID in m4: {m4.CASE_ID.nunique()}\")\n",
    "print(f\"# unique PARENT_CASEs in m4: {m4.PARENT_CASE.nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "m5 = (\n",
    "    m4.drop(columns = ['PROP_ID', 'parent_is_null'])\n",
    "    # Nothing dropped here, but just in case\n",
    "    .drop_duplicates()\n",
    "    .sort_values(['CASE_ID', 'AIN'])\n",
    "    .reset_index(drop=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# obs in m5: 349897\n",
      "# unique CASE_ID in m5: 49844\n",
      "# unique PARENT_CASEs in m5: 47046\n"
     ]
    }
   ],
   "source": [
    "print(f\"# obs in m5: {len(m5)}\")\n",
    "print(f\"# unique CASE_ID in m5: {m5.CASE_ID.nunique()}\")\n",
    "print(f\"# unique PARENT_CASEs in m5: {m5.PARENT_CASE.nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
