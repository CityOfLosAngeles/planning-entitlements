{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entitlements in TOC-eligible parcels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import intake\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "catalog = intake.open_catalog(\"../catalogs/*.yml\")\n",
    "\n",
    "s3 = boto3.client('s3')\n",
    "bucket_name = 'city-planning-entitlements'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare methods 1 and 2 to see why parcels with entitlements differ so much"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original method\n",
    "\"\"\"\n",
    "parcels = gpd.read_file(f'zip+s3://{bucket_name}/gis/intermediate/la_parcels_toc.zip')\n",
    "\n",
    "display(parcels.TOC_Tier.value_counts())\n",
    "parcels = parcels[parcels.TOC_Tier > 0]\n",
    "\n",
    "# Upload just the parcels in TOC Tiers into S3\n",
    "parcels.to_file(driver = 'GeoJSON', filename = '../gis/intermediate/toc_eligible_parcels.geojson')\n",
    "\n",
    "s3.upload_file('../gis/intermediate/toc_eligible_parcels.geojson', f'{bucket_name}', \n",
    "               'gis/intermediate/toc_eligible_parcels.geojson')\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parcels\n",
    "* Figure out how many are duplicates\n",
    "* Won't know which AINs are used in PCTS, so keep all of them, but have a way to identify how many obs to drop later on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "parcels = gpd.read_file(f'zip+s3://{bucket_name}/gis/raw/la_parcels.zip')\n",
    "\n",
    "toc_parcels = pd.read_parquet(f's3://{bucket_name}/data/crosswalk_toc2017_parcels.parquet')\n",
    "\n",
    "parcels = pd.merge(parcels, toc_parcels, on = 'AIN', how = 'inner', validate = '1:1').to_crs({'init':'epsg:2229'})\n",
    "display(parcels.TOC_Tier.value_counts())\n",
    "\n",
    "# Upload just the parcels in TOC Tiers into S3\n",
    "parcels.to_file(driver = 'GeoJSON', filename = '../gis/intermediate/toc_eligible_parcels_withcrosswalk.geojson')\n",
    "\n",
    "s3.upload_file('../gis/intermediate/toc_eligible_parcels_withcrosswalk.geojson', \n",
    "               f'{bucket_name}', 'gis/intermediate/toc_eligible_parcels_withcrosswalk.geojson')\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "parcels_withcrosswalk = gpd.read_file(f's3://{bucket_name}/gis/intermediate/toc_eligible_parcels_withcrosswalk.geojson')\n",
    "\n",
    "# Grab the centroids and count number of duplicate obs\n",
    "parcels2_withcrosswalk = utils.get_centroid(parcels_withcrosswalk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parcels = gpd.read_file(f's3://{bucket_name}/gis/intermediate/toc_eligible_parcels.geojson')\n",
    "\n",
    "# Grab the centroids and count number of duplicate obs\n",
    "parcels2 = utils.get_centroid(parcels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subset to eligible zones and see which TOC-eligible parcels also fall in eligible zones\n",
    "zoning = gpd.read_file(f's3://{bucket_name}/gis/raw/parsed_zoning.geojson')\n",
    "\n",
    "eligible_zones = ['R2', 'R3', 'RAS3', 'R4', 'RAS4', 'R5', \n",
    "              'RD1.5', 'RD2', 'RD3', 'RD4', 'RD5', 'RD6', \n",
    "              'C1', 'C2', 'C4', 'C5']\n",
    "\n",
    "eligible_zoning = zoning[zoning.zone_class.isin(eligible_zones)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parcels_with_zoning = gpd.sjoin(parcels2, eligible_zoning, \n",
    "                            how = 'inner', op = 'intersects').drop(columns = ['index_right'])\n",
    "\n",
    "parcels_with_zoning.to_file(driver = 'GeoJSON', filename = '../gis/parcels_with_zoning.geojson')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parcels_with_zoning_withcrosswalk = gpd.sjoin(parcels2_withcrosswalk, eligible_zoning, \n",
    "                                 how = 'inner', op = 'intersects').drop(columns = ['index_right'])\n",
    "\n",
    "parcels_with_zoning_withcrosswalk.to_file(driver = 'GeoJSON', \n",
    "                                          filename = '../gis/parcels_with_zoning_withcrosswalk.geojson')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parcels_with_zoning = gpd.read_file('../gis/parcels_with_zoning.geojson')\n",
    "parcels_with_zoning_withcrosswalk = gpd.read_file('..../gis/parcels_with_zoning_withcrosswalk.geojson')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process PCTS\n",
    "* Subset and merge together tables within PCTS\n",
    "* Join parcels to zoning\n",
    "* Subset for eligible zones and eligible PCTS prefixes to see how many TOC-eligible parcels fall into eligible zones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_pcts():\n",
    "    # Grab tables from PCTS\n",
    "    cases = pd.read_parquet('../data/tCASE.parquet')\n",
    "    app = pd.read_parquet('../data/tAPLC.parquet')\n",
    "    geo_info = pd.read_parquet('../data/tPROP_GEO_INFO.parquet')\n",
    "    la_prop = pd.read_parquet('../data/tLA_PROP.parquet')\n",
    "    #cases = catalog.pcts.tCASE.read()\n",
    "    #app = catalog.pcts.tAPLC.read()\n",
    "    #geo_info = catalog.pcts.tPROP_GEO_INFO.read()\n",
    "    #la_prop = catalog.pcts.tLA_PROP.read()\n",
    "    \n",
    "    # Subset dataframes before merging\n",
    "    cases = cases.assign(\n",
    "        # Grab the year-month from received date\n",
    "        CASE_FILE_DATE = pd.to_datetime(cases['CASE_FILE_RCV_DT']).dt.to_period('M'),\n",
    "    )\n",
    "    # Subset to Oct 2017 and after    \n",
    "    cases = cases[cases.CASE_FILE_DATE >= '2017-10'][['CASE_ID', 'APLC_ID', 'CASE_NBR', \n",
    "                                                      'CASE_SEQ_NBR', 'CASE_YR_NBR', 'CASE_ACTION_ID', \n",
    "                                                      'CASE_FILE_RCV_DT', 'CASE_FILE_DATE']]    \n",
    "    \n",
    "    app = app[['APLC_ID', 'PROJ_DESC_TXT']]\n",
    "    geo_info = geo_info[['CASE_ID', 'PROP_ID']]\n",
    "    la_prop = la_prop[la_prop.ASSR_PRCL_NBR.notna()][['PROP_ID', 'ASSR_PRCL_NBR']]\n",
    "    parents = pd.read_parquet(f's3://{bucket_name}/data/intermediate/parent_cases.parquet')\n",
    "\n",
    "    # Merge in parent cases\n",
    "    cases2 = pd.merge(cases, parents, on = 'CASE_ID', how = 'inner', validate = '1:1')\n",
    "    \n",
    "    # Merge with geo_info, la_prop, parcels to ID the parcels that have entitlements (10/2017 and after)\n",
    "    m1 = pd.merge(cases2, geo_info, on = 'CASE_ID', how = 'inner', validate = '1:m')\n",
    "    m2 = pd.merge(m1, la_prop, on = 'PROP_ID', how = 'inner', validate = 'm:1')\n",
    "    m3 = pd.merge(m2, parcels, left_on = 'ASSR_PRCL_NBR', right_on = 'AIN', how = 'inner', validate = 'm:1')\n",
    "    m4 = pd.merge(m3, app, on = 'APLC_ID', how = 'left', validate = 'm:1')\n",
    "    \n",
    "    m4 = m4.assign(\n",
    "        id = m4.CASE_SEQ_NBR.astype(int).astype(str) + '_' + m4.CASE_YR_NBR.astype(int).astype(str)\n",
    "    )  \n",
    "  \n",
    "    m4 = m4.drop_duplicates(subset = ['id', 'CASE_FILE_DATE', 'AIN', 'TOC_Tier', 'PROJ_DESC_TXT'])\n",
    "\n",
    "    return m4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zoning_pcts_processing(df, parcels_with_zoning): \n",
    "    # Merge in zoning and TOC info about the parcel\n",
    "    m1 = pd.merge(pcts, parcels_with_zoning, on = ['AIN', 'centroid', 'TOC_Tier'], how = 'inner')\n",
    "\n",
    "    # Drop duplicates\n",
    "    m1 = m1.drop_duplicates()\n",
    "\n",
    "    # Parse PCTS string and grab prefix\n",
    "    parsed_col_names = ['prefix']\n",
    "\n",
    "    def parse_pcts(row):\n",
    "        try:\n",
    "            z = utils.PCTSCaseNumber(row.CASE_NBR)\n",
    "            return pd.Series([z.prefix], index = parsed_col_names)\n",
    "        except ValueError:\n",
    "            return pd.Series([z.prefix], index = parsed_col_names)\n",
    "\n",
    "    parsed = m1.apply(parse_pcts, axis = 1)\n",
    "    m2 = pd.concat([m1, parsed], axis = 1)\n",
    "    \n",
    "   \n",
    "    # Subset by PCTS prefix, drop ENV/ADM/PAR cases\n",
    "    drop_prefix = ['ENV', 'ADM', 'PAR']\n",
    "    m3 = m2.loc[~m2.prefix.isin(drop_prefix)]\n",
    "\n",
    "    # Subset by CASE_ACTION_ID -- let's use all cases for now (but approved cases are 1, 2, 11)\n",
    "\n",
    "    # At this point, no more duplicates by PARENT_CASE - AIN combination\n",
    "    return m3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tag_toc_entitlements(df):\n",
    "    keep_col = ['CASE_NBR', 'id', 'CASE_ACTION_ID', 'CASE_FILE_DATE', \n",
    "            'AIN', 'TOC_Tier', 'zone_class']\n",
    "    \n",
    "    df = (df[keep_col]\n",
    "          .assign(is_TOC = df.CASE_NBR.str.contains('TOC').astype(int))\n",
    "         )\n",
    "    \n",
    "    # Make into parcel-level df\n",
    "    df2 = (df.groupby(['AIN', 'TOC_Tier', 'zone_class', 'is_TOC'])\n",
    "           .agg({'id':'count'})\n",
    "           .reset_index()) \n",
    "\n",
    "    # Make wide\n",
    "    df2 = df2.assign(\n",
    "        num_TOC = df2.apply(lambda row: row.id if row.is_TOC == 1 else np.nan, axis = 1),\n",
    "        num_nonTOC = df2.apply(lambda row: row.id if row.is_TOC == 0 else np.nan, axis = 1)\n",
    "    )\n",
    "    \n",
    "    \n",
    "    # If there are multiple obs for the same AIN, fill the NaNs with the max from the other column \n",
    "    # Then, drop duplicates\n",
    "    df2 = df2.assign(\n",
    "        num_TOC = df2.num_TOC.fillna(df2.groupby('AIN')['num_TOC'].transform('max')),\n",
    "        num_nonTOC = df2.num_nonTOC.fillna(df2.groupby('AIN')['num_nonTOC'].transform('max'))\n",
    "    )\n",
    "    \n",
    "    df3 = df2.drop_duplicates(subset = ['AIN', 'TOC_Tier', 'zone_class', 'num_TOC', 'num_nonTOC'])\n",
    "\n",
    "    df3 = (df3.assign(\n",
    "            num_TOC = df3.num_TOC.fillna(0).astype(int),\n",
    "            num_nonTOC = df3.num_nonTOC.fillna(0).astype(int)\n",
    "        ).drop(columns = ['is_TOC', 'id'])\n",
    "    )\n",
    "    \n",
    "    # Merge in centroids for these parcels (much easier to plot)\n",
    "    df4 = pd.merge(df3, parcels2, on = ['AIN', 'TOC_Tier'], how = 'inner').drop(\n",
    "                    columns = ['x', 'y', 'obs', 'num_obs'])\n",
    "    \n",
    "    df4.rename(columns = {'centroid':'geometry'}, inplace = True)\n",
    "    df4 = gpd.GeoDataFrame(df4)\n",
    "    df4.crs = {'init':'epsg:2229'}\n",
    "\n",
    "    return df4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcts = merge_pcts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcts2 = zoning_pcts_processing(pcts, parcels_with_zoning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcts2_withcrosswalk = zoning_pcts_processing(pcts, parcels_with_zoning_withcrosswalk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcts3 = tag_toc_entitlements(pcts2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcts3_withcrosswalk = tag_toc_entitlements(pcts2_withcrosswalk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toc_parcels = pcts3[pcts3.num_TOC > 0]\n",
    "non_toc_parcels = pcts3[pcts3.num_nonTOC > 0]\n",
    "have_both_parcels = pcts3[(pcts3.num_TOC > 0) & (pcts3.num_nonTOC > 0)]\n",
    "\n",
    "print(f'# parcels: {len(pcts3)}')\n",
    "print(f'# parcels with TOC entitlements: {len(toc_parcels)}')\n",
    "print(f'# parcels with non TOC entitlements: {len(non_toc_parcels)}')\n",
    "print(f'# parcels with both TOC and non TOC entitlements: {len(have_both_parcels)}')\n",
    "print(f'double check sum: {len(toc_parcels) + len(non_toc_parcels) - len(have_both_parcels)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toc_parcels = pcts3_withcrosswalk[pcts3_withcrosswalk.num_TOC > 0]\n",
    "non_toc_parcels = pcts3_withcrosswalk[pcts3_withcrosswalk.num_nonTOC > 0]\n",
    "have_both_parcels = pcts3_withcrosswalk[(pcts3_withcrosswalk.num_TOC > 0) & (pcts3_withcrosswalk.num_nonTOC > 0)]\n",
    "\n",
    "print(f'# parcels: {len(pcts3_withcrosswalk)}')\n",
    "print(f'# parcels with TOC entitlements: {len(toc_parcels)}')\n",
    "print(f'# parcels with non TOC entitlements: {len(non_toc_parcels)}')\n",
    "print(f'# parcels with both TOC and non TOC entitlements: {len(have_both_parcels)}')\n",
    "print(f'double check sum: {len(toc_parcels) + len(non_toc_parcels) - len(have_both_parcels)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toc_parcels = df[df.num_TOC > 0]\n",
    "non_toc_parcels = df[df.num_nonTOC > 0]\n",
    "have_both_parcels = df[(df.num_TOC > 0) & (df.num_nonTOC > 0)]\n",
    "\n",
    "print(f'# parcels: {len(df)}')\n",
    "print(f'# parcels with TOC entitlements: {len(toc_parcels)}')\n",
    "print(f'# parcels with non TOC entitlements: {len(non_toc_parcels)}')\n",
    "print(f'# parcels with both TOC and non TOC entitlements: {len(have_both_parcels)}')\n",
    "print(f'double check sum: {len(toc_parcels) + len(non_toc_parcels) - len(have_both_parcels)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'% parcels with TOC entitlements: {len(toc_parcels) / len(df)}')\n",
    "print(f'% parcels with non TOC entitlements: {len(non_toc_parcels) / len(df)}')\n",
    "print(f'% parcels with both entitlements: {len(have_both_parcels) / len(df)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.TOC_Tier.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toc_parcels.zone_class.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_toc_parcels.zone_class.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "m12.to_file(driver = 'GeoJSON', filename = '../gis/intermediate/toc_eligible_parcels_with_entitlements.geojson')\n",
    "\n",
    "s3.upload_file('../gis/intermediate/toc_eligible_parcels_with_entitlements.geojson', \n",
    "               f'{bucket_name}', 'gis/intermediate/toc_eligible_parcels_with_entitlements.geojson')\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Breakdown by TOC Tiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "m13 = m12.groupby('TOC_Tier').agg({'AIN':'count', 'num_TOC':'sum', 'num_nonTOC':'sum'}).reset_index()\n",
    "\n",
    "for i in ['TOC', 'nonTOC']:\n",
    "    new_col = f'pct_{i}'\n",
    "    numerator = f'num_{i}'\n",
    "    m13[new_col] = m13[numerator] / (m13.num_TOC + m13.num_nonTOC)\n",
    "    \n",
    "m13['all_AIN'] = m13.AIN.sum()\n",
    "m13['pct_AIN'] = m13.AIN / m13.all_AIN\n",
    "\n",
    "m13\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Breakdown by Zone Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "m14 = m12.groupby('zone_class').agg({'AIN': 'count', 'num_TOC': 'sum', 'num_nonTOC': 'sum'}).reset_index()\n",
    "\n",
    "for i in ['TOC', 'nonTOC']:\n",
    "    new_col = f'pct_{i}'\n",
    "    numerator = f'num_{i}'\n",
    "    m14[new_col] = m14[numerator] / (m14.num_TOC + m14.num_nonTOC)\n",
    "    \n",
    "m14['all_AIN'] = m14.AIN.sum()\n",
    "m14['pct_AIN'] = m14.AIN / m14.all_AIN\n",
    "\n",
    "m14\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "writer = pd.ExcelWriter('../outputs/toc_charts.xlsx', engine = 'xlsxwriter')\n",
    "\n",
    "m13.to_excel(writer, sheet_name = 'entitlements_by_tier')\n",
    "m14.to_excel(writer, sheet_name = 'entitlements_by_zone')\n",
    "\n",
    "writer.save()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
