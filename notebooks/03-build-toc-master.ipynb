{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entitlements in TOC-eligible parcels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import geopandas as gpd\n",
    "import intake\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pcts_parser\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "catalog = intake.open_catalog(\"../catalogs/*.yml\")\n",
    "\n",
    "s3 = boto3.client('s3')\n",
    "bucket_name = 'city-planning-entitlements'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parcels\n",
    "* Won't know which AINs are used inpcts.CASE_NBRep all of them, but have a way to identify how many obs to drop later on\n",
    "* Use the PCTS util to get PCTS data, then subset to TOC only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_tract_level(start_date):    \n",
    "    # Import data\n",
    "    pcts = pd.read_parquet(f's3://{bucket_name}/data/final/master_pcts.parquet')\n",
    "    parents = pd.read_parquet(f's3://{bucket_name}/data/final/parents_with_suffix.parquet')\n",
    "    crosswalk_parcels_tracts = pd.read_parquet(f's3://{bucket_name}/data/crosswalk_parcels_tracts.parquet')\n",
    "    \n",
    "    # Subset PCTS, we'll start with last 10 years\n",
    "    pcts = pcts[pcts.CASE_FILE_DATE >= start_date]\n",
    "    \n",
    "    # (1a) Only keep parent cases\n",
    "    m1 = pd.merge(pcts, parents, on = 'PARENT_CASE', how = 'inner', validate = 'm:1')\n",
    "\n",
    "    # (1b) Make cases parcel-level\n",
    "    parents_by_parcel = (m1.groupby(['AIN'])\n",
    "                         .agg({'PARENT_CASE':'count'})\n",
    "                         .reset_index()\n",
    "                         .rename(columns = {'PARENT_CASE':'num_cases'})\n",
    "                        )\n",
    "\n",
    "    # (1c) Merge in tract info and aggregate to tract-level\n",
    "    m2 = pd.merge(parents_by_parcel, crosswalk_parcels_tracts, on = 'AIN', how = 'inner', validate = '1:1')\n",
    "\n",
    "    # Even though the column num_AIN shows there are some parcels with more than 1 obs,\n",
    "    # once we merged in parent cases, no AIN shows up more than once\n",
    "\n",
    "    # (1d) Subset by prefixes?\n",
    "    def subset_by_prefix_suffix(row):\n",
    "        if any(p in row.CASE_NBR for p in prefix_or_suffix_list):\n",
    "            return True\n",
    "        else: \n",
    "            return False \n",
    "    \n",
    "    ent_by_tract = m2.groupby(['GEOID', 'pop']).agg({'num_cases':'sum', 'AIN':'count'}).reset_index()\n",
    "    \n",
    "    \n",
    "    # (2a) Only keep suffixes\n",
    "    suffix = m1.loc[:, '1A':'ZV']\n",
    "    \n",
    "    m3 = pd.merge(m1[['AIN']], suffix, left_index = True, right_index = True)\n",
    "\n",
    "    # (2b) Make suffixes parcel-level\n",
    "    suffix_by_parcel = (m3.pivot_table(index = 'AIN', aggfunc = 'sum')\n",
    "                        .reset_index()\n",
    "                       )\n",
    "\n",
    "    # (2c) Merge in tract info and aggregate to tract-level\n",
    "    m4 = pd.merge(suffix_by_parcel, crosswalk_parcels_tracts, \n",
    "                                 on = 'AIN', how = 'left', validate = '1:1')\n",
    "\n",
    "    # Aggregate the number of suffixes by tract\n",
    "    suffix_by_tract = m4.pivot_table(index = ['GEOID', 'pop'], aggfunc = 'sum').reset_index()\n",
    "    \n",
    "    \n",
    "    # (3) Merge number of cases and suffixes by tract\n",
    "    df = pd.merge(ent_by_tract, suffix_by_tract, on = ['GEOID', 'pop'], how = 'left', validate = '1:1')\n",
    "        \n",
    "    # (4) Make sure everything returns as integers and not floats\n",
    "    colnames = list(df.columns)\n",
    "\n",
    "    for r in ['GEOID', 'pop', 'AIN']:\n",
    "        colnames.remove(r)\n",
    "    \n",
    "    # Count number of suffixes, since each ENT can have any number of suffixes involved\n",
    "    df[colnames] = df[colnames].fillna(0).astype(int) \n",
    "    df['num_suffix'] = df[colnames].sum(axis=1)\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = \"2017-10\"\n",
    "pcts = make_tract_level(start_date)\n",
    "\n",
    "# Just keep TOC ent for the tract\n",
    "keep = ['GEOID', 'pop', 'num_cases', 'AIN', 'num_AIN', 'parcel_tot', 'parcelsqft', 'TOC',]\n",
    "pcts = pcts[keep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prep census tract data to make map\n",
    "census = catalog.census_tracts.read()\n",
    "census = (census[['GEOID10', 'geometry']]\n",
    "        .assign(num = 0)\n",
    "        .rename(columns = {'GEOID10':'GEOID'})\n",
    "         ).to_crs(\"EPSG:4326\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a map showing TOC ent by tract, but also show all census tracts in LA\n",
    "gdf = pd.merge(census.drop(columns = 'num'), pcts, on = 'GEOID', how = 'inner', validate = '1:1')\n",
    "gdf = gdf[gdf.TOC > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipyleaflet\n",
    "from ipyleaflet import Map, GeoData, LayersControl, basemaps, WidgetControl\n",
    "from ipywidgets import link, FloatSlider, Text, HTML\n",
    "from ipywidgets.embed import embed_minimal_html\n",
    "import branca.colormap\n",
    "import json\n",
    "\n",
    "geo_data = json.loads(gdf.set_index('GEOID').to_json())\n",
    "\n",
    "# Take what we want to map and turn it into a dictionary\n",
    "# Can only include the key-value pair, the value you want to map, nothing more.\n",
    "df = dict(zip(gdf['GEOID'].tolist(), gdf['TOC'].tolist()))\n",
    "\n",
    "\n",
    "census_geo = json.loads(census.set_index('GEOID').to_json())\n",
    "census_df = dict(zip(census['GEOID'].tolist(), census['num'].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ceb61e9ee21497d8eafc585407e31e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map(center=[34.0536, -118.2427], controls=(ZoomControl(options=['position', 'zoom_in_text', 'zoom_in_title', 'â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "m = ipyleaflet.Map(center = (34.0536, -118.2427), zoom = 11,\n",
    "                  basemap = basemaps.CartoDB.Positron)\n",
    "\n",
    "census_tracts = ipyleaflet.Choropleth(\n",
    "    geo_data = census_geo,\n",
    "    choro_data = census_df, \n",
    "    colormap = branca.colormap.LinearColormap([\n",
    "        \"#FFFFFF\",\n",
    "        \"#FFFFFF\",        \n",
    "    ]),\n",
    "    border_color = '#999999',\n",
    "    style = {'fillOpacity': 0.4, 'weight': 0.5, 'color': '#999999', 'opacity': 0.8},\n",
    "    value_min = 0,\n",
    "    value_max = 0,\n",
    "    name = 'Census Tracts'\n",
    ")\n",
    "\n",
    "toc = ipyleaflet.Choropleth(\n",
    "    geo_data = geo_data,\n",
    "    choro_data = df,\n",
    "    colormap = branca.colormap.LinearColormap([\n",
    "        \"#C9FFFD\",\n",
    "        \"#06BEE1\",\n",
    "        \"#1768AC\",\n",
    "        \"#2541B2\",\n",
    "        \"#03256C\"\n",
    "    ]),\n",
    "    border_color = 'white',\n",
    "    style = {'fillOpacity': 0.8, 'weight': 0.5, 'color': 'white', 'opacity': 0.8},\n",
    "    hover_style = {'fillOpacity': 0.85},\n",
    "    value_min = 0,\n",
    "    value_max = 60,\n",
    "    name = 'TOC entitlements'\n",
    ")\n",
    "\n",
    "html = HTML(''' \n",
    "    Hover over a tract\n",
    "''')\n",
    "\n",
    "html.layout.margin = '0 px 10px 10px 10px'\n",
    "\n",
    "def click_handler(event = None, id = None, properties = None):\n",
    "    label.value = properties['TOC']\n",
    "\n",
    "def update_html_tract(feature, id, **kwargs): \n",
    "    html.value = '''\n",
    "        Census Tract:  \n",
    "        <b>{}</b> <br>\n",
    "        Number of TOC Entitlements (2017-2019):\n",
    "        {} \n",
    "    '''.format(id, feature['properties']['TOC'])    \n",
    "    \n",
    "toc.on_click(click_handler) \n",
    "toc.on_hover(update_html_tract)\n",
    "\n",
    "m.add_layer(census_tracts)\n",
    "m.add_layer(toc)\n",
    "\n",
    "control = ipyleaflet.WidgetControl(widget = html, position = 'topright')\n",
    "layers_control = ipyleaflet.LayersControl(position = 'topright')\n",
    "\n",
    "m.add_control(control)\n",
    "m.add_control(layers_control)\n",
    "\n",
    "m.layout.height = '100%'\n",
    "m.layout.min_height = '600px'\n",
    "\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parcels = gpd.read_file(f'zip+s3://{bucket_name}/gis/intermediate/TOC_Parcels.zip')\n",
    "\n",
    "# Grab the centroids and count number of duplicate obs\n",
    "parcels2 = utils.get_centroid(parcels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subset to eligible zones and see which TOC-eligible parcels also fall in eligible zones\n",
    "zoning = gpd.read_file(f's3://{bucket_name}/gis/raw/parsed_zoning.geojson')\n",
    "\n",
    "eligible_zones = ['R2', 'R3', 'RAS3', 'R4', 'RAS4', 'R5', \n",
    "              'RD1.5', 'RD2', 'RD3', 'RD4', 'RD5', 'RD6', \n",
    "              'C1', 'C2', 'C4', 'C5']\n",
    "\n",
    "eligible_zoning = zoning[zoning.zone_class.isin(eligible_zones)]\n",
    "\n",
    "parcels_with_zoning = gpd.sjoin(parcels2, eligible_zoning, \n",
    "                                how = 'inner', op = 'intersects').drop(columns = ['index_right'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process PCTS\n",
    "* Join parcels to zoning\n",
    "* Subset for eligible zones and eligible PCTS prefixes to see how many TOC-eligible parcels fall into eligible zones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zoning_pcts_processing(df, parcels_with_zoning): \n",
    "    # Merge in zoning and TOC info about the parcel\n",
    "    m1 = pd.merge(df, parcels_with_zoning, on = ['AIN'], how = 'inner')\n",
    "    \n",
    "    # Drop duplicates\n",
    "    m1 = m1.drop_duplicates()\n",
    "\n",
    "    # Parse PCTS string and grab prefix\n",
    "    parsed_col_names = ['prefix']\n",
    "\n",
    "    def parse_pcts(row):\n",
    "        try:\n",
    "            z = pcts_parser.PCTSCaseNumber(row.CASE_NBR)\n",
    "            return pd.Series([z.prefix], index = parsed_col_names)\n",
    "        except ValueError:\n",
    "            return pd.Series([z.prefix], index = parsed_col_names)\n",
    "\n",
    "    parsed = m1.apply(parse_pcts, axis = 1)\n",
    "    m2 = pd.concat([m1, parsed], axis = 1)\n",
    "    \n",
    "   \n",
    "    # Subset by PCTS prefix, drop ENV/ADM/PAR cases\n",
    "    drop_prefix = ['ENV', 'ADM', 'PAR']\n",
    "    m3 = m2.loc[~m2.prefix.isin(drop_prefix)]\n",
    "    \n",
    "    # Subset by CASE_ACTION_ID -- let's use all cases for now (but approved cases are 1, 2, 11)\n",
    "    approved_cases = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
    "    m4 = m3.loc[m3.CASE_ACTION_ID.isin(approved_cases)]\n",
    "        \n",
    "    # At this point, no more duplicates by PARENT_CASE - AIN combination\n",
    "    return m4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tag_toc_entitlements(df):\n",
    "    keep_col = ['CASE_NBR', 'id', 'CASE_ACTION_ID', 'CASE_FILE_DATE', \n",
    "            'AIN', 'TOC_Tier', 'zone_class']\n",
    "    \n",
    "    df = (df[keep_col]\n",
    "          .assign(is_TOC = df.CASE_NBR.str.contains('TOC').astype(int))\n",
    "         )\n",
    "    \n",
    "    # Make into parcel-level df\n",
    "    df2 = (df.groupby(['AIN', 'TOC_Tier', 'zone_class', 'is_TOC'])\n",
    "           .agg({'id':'count'})\n",
    "           .reset_index()) \n",
    "\n",
    "    # Make wide\n",
    "    df2 = df2.assign(\n",
    "        num_TOC = df2.apply(lambda row: row.id if row.is_TOC == 1 else np.nan, axis = 1),\n",
    "        num_nonTOC = df2.apply(lambda row: row.id if row.is_TOC == 0 else np.nan, axis = 1)\n",
    "    )\n",
    "    \n",
    "    \n",
    "    # If there are multiple obs for the same AIN, fill the NaNs with the max from the other column \n",
    "    # Then, drop duplicates\n",
    "    df2 = df2.assign(\n",
    "        num_TOC = df2.num_TOC.fillna(df2.groupby('AIN')['num_TOC'].transform('max')),\n",
    "        num_nonTOC = df2.num_nonTOC.fillna(df2.groupby('AIN')['num_nonTOC'].transform('max'))\n",
    "    )\n",
    "    \n",
    "    df3 = df2.drop_duplicates(subset = ['AIN', 'TOC_Tier', 'zone_class', 'num_TOC', 'num_nonTOC'])\n",
    "\n",
    "    df3 = (df3.assign(\n",
    "            num_TOC = df3.num_TOC.fillna(0).astype(int),\n",
    "            num_nonTOC = df3.num_nonTOC.fillna(0).astype(int)\n",
    "        ).drop(columns = ['is_TOC', 'id'])\n",
    "    )\n",
    "    \n",
    "    # Merge in centroids for these parcels (much easier to plot)\n",
    "    df4 = pd.merge(df3, parcels2, on = ['AIN', 'TOC_Tier'], how = 'inner').drop(\n",
    "                    columns = ['x', 'y', 'obs', 'num_obs'])\n",
    "    \n",
    "    df4.rename(columns = {'centroid':'geometry'}, inplace = True)\n",
    "    df4 = gpd.GeoDataFrame(df4)\n",
    "    df4.crs = {'init':'epsg:2229'}\n",
    "\n",
    "    return df4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcts2 = zoning_pcts_processing(pcts, parcels_with_zoning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = tag_toc_entitlements(pcts2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toc_parcels = df[df.num_TOC > 0]\n",
    "non_toc_parcels = df[df.num_nonTOC > 0]\n",
    "have_both_parcels = df[(df.num_TOC > 0) & (df.num_nonTOC > 0)]\n",
    "\n",
    "print(f'# parcels: {len(df)}')\n",
    "print(f'# parcels with TOC entitlements: {len(toc_parcels)}')\n",
    "print(f'# parcels with non TOC entitlements: {len(non_toc_parcels)}')\n",
    "print(f'# parcels with both TOC and non TOC entitlements: {len(have_both_parcels)}')\n",
    "print(f'double check sum: {len(toc_parcels) + len(non_toc_parcels) - len(have_both_parcels)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'% parcels with TOC entitlements: {len(toc_parcels) / len(df)}')\n",
    "print(f'% parcels with non TOC entitlements: {len(non_toc_parcels) / len(df)}')\n",
    "print(f'% parcels with both entitlements: {len(have_both_parcels) / len(df)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toc_parcels.zone_class.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_toc_parcels.zone_class.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_file(driver = 'GeoJSON', filename = '../gis/intermediate/toc_eligible_parcels_with_entitlements.geojson')\n",
    "\n",
    "s3.upload_file('../gis/intermediate/toc_eligible_parcels_with_entitlements.geojson', \n",
    "               f'{bucket_name}', 'gis/intermediate/toc_eligible_parcels_with_entitlements.geojson')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Breakdown by TOC Tiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_by_tiers(df):\n",
    "    df2 = df.groupby('TOC_Tier').agg({'AIN':'count', 'num_TOC':'sum', 'num_nonTOC':'sum'}).reset_index()\n",
    "    \n",
    "    for i in ['TOC', 'nonTOC']:\n",
    "        new_col = f'pct_{i}'\n",
    "        numerator = f'num_{i}'\n",
    "        df2[new_col] = df2[numerator] / (df2.num_TOC + df2.num_nonTOC)\n",
    "    \n",
    "    df2['all_AIN'] = df2.AIN.sum()\n",
    "    df2['pct_AIN'] = df2.AIN / df2.all_AIN\n",
    "    \n",
    "    return df2\n",
    "\n",
    "by_tiers = summarize_by_tiers(df)\n",
    "by_tiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Breakdown by Zone Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_by_zones(df):\n",
    "    df2 = df.groupby('zone_class').agg({'AIN':'count', 'num_TOC':'sum', 'num_nonTOC':'sum'}).reset_index()\n",
    "    \n",
    "    for i in ['TOC', 'nonTOC']:\n",
    "        new_col = f'pct_{i}'\n",
    "        numerator = f'num_{i}'\n",
    "        df2[new_col] = df2[numerator] / (df2.num_TOC + df2.num_nonTOC)\n",
    "    \n",
    "    df2['all_AIN'] = df2.AIN.sum()\n",
    "    df2['pct_AIN'] = df2.AIN / df2.all_AIN\n",
    "    \n",
    "    return df2\n",
    "\n",
    "by_zones = summarize_by_zones(df)\n",
    "by_zones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = pd.ExcelWriter('../outputs/toc_charts.xlsx', engine = 'xlsxwriter')\n",
    "\n",
    "by_tiers.to_excel(writer, sheet_name = 'entitlements_by_tier')\n",
    "by_zones.to_excel(writer, sheet_name = 'entitlements_by_zone')\n",
    "\n",
    "writer.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
