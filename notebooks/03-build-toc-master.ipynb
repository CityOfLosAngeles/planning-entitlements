{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entitlements in TOC-eligible parcels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import intake\n",
    "import boto3\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "catalog = intake.open_catalog(\"../catalogs/*.yml\")\n",
    "\n",
    "s3 = boto3.client('s3')\n",
    "bucket_name = 'city-planning-entitlements'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parcels\n",
    "* Figure out how many are duplicates\n",
    "* Won't know which AINs are used in PCTS, so keep all of them, but have a way to identify how many obs to drop later on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "parcels = gpd.read_file(f'zip+s3://{bucket_name}/gis/raw/la_parcels.zip')\n",
    "\n",
    "toc_parcels = pd.read_parquet(f's3://{bucket_name}/data/crosswalk_toc2017_parcels.parquet')\n",
    "\n",
    "parcels = pd.merge(parcels, toc_parcels, on = 'AIN', how = 'inner', validate = '1:1').to_crs({'init':'epsg:2229'})\n",
    "display(parcels.TOC_Tier.value_counts())\n",
    "\n",
    "# Upload just the parcels in TOC Tiers into S3\n",
    "parcels.to_file(driver = 'GeoJSON', filename = '../gis/intermediate/toc_eligible_parcels.geojson')\n",
    "\n",
    "s3.upload_file('../gis/intermediate/toc_eligible_parcels.geojson', \n",
    "               f'{bucket_name}', 'gis/intermediate/toc_eligible_parcels.geojson')\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AIN</th>\n",
       "      <th>TOC_Tier</th>\n",
       "      <th>centroid</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>obs</th>\n",
       "      <th>num_obs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2010004040</td>\n",
       "      <td>1</td>\n",
       "      <td>POINT (6378795.418 1908220.195)</td>\n",
       "      <td>6.378795e+06</td>\n",
       "      <td>1.908220e+06</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024023012</td>\n",
       "      <td>2</td>\n",
       "      <td>POINT (6373279.994 1893585.043)</td>\n",
       "      <td>6.373280e+06</td>\n",
       "      <td>1.893585e+06</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          AIN  TOC_Tier                         centroid             x  \\\n",
       "0  2010004040         1  POINT (6378795.418 1908220.195)  6.378795e+06   \n",
       "1  2024023012         2  POINT (6373279.994 1893585.043)  6.373280e+06   \n",
       "\n",
       "              y  obs  num_obs  \n",
       "0  1.908220e+06    1        1  \n",
       "1  1.893585e+06    1        1  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parcels = gpd.read_file(f's3://{bucket_name}/gis/intermediate/toc_eligible_parcels.geojson')\n",
    "\n",
    "# Grab the centroids and count number of duplicate obs\n",
    "parcels2 = utils.get_centroid(parcels)\n",
    "parcels2.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process PCTS\n",
    "* Subset and merge together tables within PCTS\n",
    "* Join parcels to zoning\n",
    "* Subset for eligible zones and eligible PCTS prefixes to see how many TOC-eligible parcels fall into eligible zones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_pcts():\n",
    "    # Grab tables from PCTS\n",
    "    cases = pd.read_parquet('../data/tCASE.parquet')\n",
    "    app = pd.read_parquet('../data/tAPLC.parquet')\n",
    "    geo_info = pd.read_parquet('../data/tPROP_GEO_INFO.parquet')\n",
    "    la_prop = pd.read_parquet('../data/tLA_PROP.parquet')\n",
    "    #cases = catalog.pcts.tCASE.read()\n",
    "    #app = catalog.pcts.tAPLC.read()\n",
    "    #geo_info = catalog.pcts.tPROP_GEO_INFO.read()\n",
    "    #la_prop = catalog.pcts.tLA_PROP.read()\n",
    "    \n",
    "    # Subset dataframes before merging\n",
    "    cases = cases.assign(\n",
    "        # Grab the year-month from received date\n",
    "        CASE_FILE_DATE = pd.to_datetime(cases['CASE_FILE_RCV_DT']).dt.to_period('M'),\n",
    "    )\n",
    "    # Subset to Oct 2017 and after    \n",
    "    cases = cases[cases.CASE_FILE_DATE >= '2017-10'][['CASE_ID', 'APLC_ID', 'CASE_NBR', \n",
    "                                                      'CASE_SEQ_NBR', 'CASE_YR_NBR', 'CASE_ACTION_ID', \n",
    "                                                      'CASE_FILE_RCV_DT', 'CASE_FILE_DT']]    \n",
    "    \n",
    "    app = app[['APLC_ID', 'PROJ_DESC_TXT']]\n",
    "    geo_info = geo_info[['CASE_ID', 'PROP_ID']]\n",
    "    la_prop = la_prop[la_prop.ASSR_PRCL_NBR.notna()][['PROP_ID', 'ASSR_PRCL_NBR']]\n",
    "     \n",
    "    # Merge in parent cases\n",
    "    parents = pd.read_parquet(f's3://{bucket_name}/data/intermediate/parent_cases.parquet')\n",
    "    cases2 = pd.merge(cases, parents, on = 'CASE_ID', how = 'inner', validate = '1:1')\n",
    "    \n",
    "    # Merge with geo_info, la_prop, parcels to ID the parcels that have entitlements (10/2017 and after)\n",
    "    m1 = pd.merge(cases2, geo_info, on = 'CASE_ID', how = 'inner', validate = '1:m')\n",
    "    m2 = pd.merge(m1, la_prop, on = 'PROP_ID', how = 'inner', validate = 'm:1')\n",
    "    m3 = pd.merge(m2, parcels, left_on = 'ASSR_PRCL_NBR', right_on = 'AIN', how = 'inner', validate = 'm:1')\n",
    "    m4 = pd.merge(m3, app, on = 'APLC_ID', how = 'inner', validate = 'm:1')\n",
    "    \n",
    "    return m4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zoning_pcts_processing(df):\n",
    "    # Subset to eligible zones and see which TOC-eligible parcels also fall in eligible zones\n",
    "    zoning = gpd.read_file(f's3://{bucket_name}/gis/raw/parsed_zoning.geojson')\n",
    "    \n",
    "    eligible_zones = ['R2', 'R3', 'RAS3', 'R4', 'RAS4', 'R5', \n",
    "                  'RD1.5', 'RD2', 'RD3', 'RD4', 'RD5', 'RD6', \n",
    "                  'C1', 'C2', 'C4', 'C5']\n",
    "\n",
    "    eligible_zoning = zoning[zoning.zone_class.isin(eligible_zones)]\n",
    "    \n",
    "    parcels_with_zoning = gpd.sjoin(parcels2, eligible_zoning, \n",
    "                                how = 'inner', op = 'intersects').drop(columns = ['index_right'])\n",
    "    \n",
    "    # Merge in zoning and TOC info about the parcel\n",
    "    m1 = pd.merge(pcts, parcels_with_zoning, on = ['AIN', 'centroid', 'TOC_Tier'], how = 'inner')\n",
    "\n",
    "    # Drop duplicates\n",
    "    m1 = m1.drop_duplicates()\n",
    "    \n",
    "    # Parse PCTS string and grab prefix\n",
    "    parsed_col_names = ['prefix']\n",
    "\n",
    "    def parse_pcts(row):\n",
    "        try:\n",
    "            z = utils.PCTSCaseNumber(row.CASE_NBR)\n",
    "            return pd.Series([z.prefix], index = parsed_col_names)\n",
    "        except ValueError:\n",
    "            return pd.Series([z.prefix], index = parsed_col_names)\n",
    "\n",
    "    parsed = m1.apply(parse_pcts, axis = 1)\n",
    "    m2 = pd.concat([m1, parsed], axis = 1)\n",
    "    \n",
    "    # Create new ID variable that is just seq number and year. Need way to get rid of duplicates.\n",
    "    drop = ['centroid', 'x', 'y', 'obs', 'num_obs', \n",
    "            'CASE_ID', 'APLC_ID', 'ASSR_PRCL_NBR', 'PROP_ID', \n",
    "            'o1', 'o2', 'o3', 'o1_descrip', 'o2_descrip', 'o3_descrip']\n",
    "    \n",
    "    m2 = (m2.assign(\n",
    "            id = m2.CASE_SEQ_NBR.astype(int).astype(str) + '_' + m2.CASE_YR_NBR.astype(int).astype(str)\n",
    "        ).drop(columns = drop)\n",
    "        .reset_index()\n",
    "    )\n",
    "    \n",
    "    # Drop duplicates\n",
    "    cols_we_need = ['id', 'CASE_ACTION_ID', 'CASE_FILE_DATE', \n",
    "                    'AIN', 'TOC_Tier', 'ZONE_CMPLT', 'PROJ_DESC_TXT']   \n",
    "    m3 = m2.drop_duplicates(subset = cols_we_need)\n",
    "    \n",
    "    # Subset by PCTS prefix, drop ENV/ADM/PAR cases\n",
    "    drop_prefix = ['ENV', 'ADM', 'PAR']\n",
    "    m4 = m3.loc[~m3.prefix.isin(drop_prefix)]\n",
    "    \n",
    "    # Subset by CASE_ACTION_ID -- let's use all cases for now (but approved cases are 1, 2, 11)\n",
    "    approved_cases = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
    "\n",
    "    m5 = m4.loc[m4.CASE_ACTION_ID.isin(approved_cases)]\n",
    "    \n",
    "    # At this point, no more duplicates by PARENT_CASE - AIN combination\n",
    "    return m5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcts = merge_pcts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "Index(['CASE_FILE_DATE'], dtype='object')",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-af7c858ad11a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpcts2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzoning_pcts_processing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpcts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-5-002ed707914f>\u001b[0m in \u001b[0;36mzoning_pcts_processing\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m     45\u001b[0m     cols_we_need = ['id', 'CASE_ACTION_ID', 'CASE_FILE_DATE', \n\u001b[1;32m     46\u001b[0m                     'AIN', 'TOC_Tier', 'ZONE_CMPLT', 'PROJ_DESC_TXT']   \n\u001b[0;32m---> 47\u001b[0;31m     \u001b[0mm3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mm2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop_duplicates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcols_we_need\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0;31m# Subset by PCTS prefix, drop ENV/ADM/PAR cases\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mdrop_duplicates\u001b[0;34m(self, subset, keep, inplace)\u001b[0m\n\u001b[1;32m   4907\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4908\u001b[0m         \u001b[0minplace\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate_bool_kwarg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"inplace\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4909\u001b[0;31m         \u001b[0mduplicated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mduplicated\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkeep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4910\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4911\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mduplicated\u001b[0;34m(self, subset, keep)\u001b[0m\n\u001b[1;32m   4964\u001b[0m         \u001b[0mdiff\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mIndex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdifference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4965\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdiff\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4966\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdiff\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4967\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4968\u001b[0m         \u001b[0mvals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msubset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: Index(['CASE_FILE_DATE'], dtype='object')"
     ]
    }
   ],
   "source": [
    "pcts2 = zoning_pcts_processing(pcts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcts2['obs'] = pcts2.groupby(['PARENT_CASE', 'AIN']).cumcount() + 1\n",
    "pcts2['max_obs'] = pcts2.groupby(['PARENT_CASE', 'AIN'])['obs'].transform('max')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional subsetting for TOC-eligible parcels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tag case as TOC or not\n",
    "m8['is_TOC'] = m8.CASE_NBR.str.contains('TOC').astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keep_col = ['CASE_NBR', 'id', 'CASE_ACTION_ID', 'CASE_FILE_DATE', \n",
    "            'AIN', 'TOC_Tier', 'zone_class', 'is_TOC']\n",
    "\n",
    "m9 = m8[keep_col]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make into parcel level df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m10 = m9.groupby(['AIN', 'TOC_Tier', 'zone_class', 'is_TOC']).agg({'id':'count'}).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make wide\n",
    "m10['num_TOC'] = m10.apply(lambda row: row.id if row.is_TOC == 1 else np.nan, axis = 1) \n",
    "m10['num_nonTOC'] = m10.apply(lambda row: row.id if row.is_TOC == 0 else np.nan, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If there are multiple obs for the same AIN, fill the NaNs with the max from the other column \n",
    "# Then, do drop duplicates after\n",
    "for col in ['num_TOC', 'num_nonTOC']:\n",
    "    m10[col] = m10[col].fillna(m10.groupby('AIN')[col].transform('max'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m11 = m10.drop_duplicates(subset = ['AIN', 'TOC_Tier', 'zone_class', 'num_TOC', 'num_nonTOC'])\n",
    "\n",
    "for col in ['num_TOC', 'num_nonTOC']:\n",
    "    m11[col] = m11[col].fillna(0).astype(int)\n",
    "\n",
    "m11 = m11.drop(columns = ['is_TOC', 'id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toc_parcels = m11[m11.num_TOC > 0]\n",
    "non_toc_parcels = m11[m11.num_nonTOC > 0]\n",
    "have_both_parcels = m11[(m11.num_TOC > 0) & (m11.num_nonTOC > 0)]\n",
    "\n",
    "print(f'# parcels: {len(m11)}')\n",
    "print(f'# parcels with TOC entitlements: {len(toc_parcels)}')\n",
    "print(f'# parcels with non TOC entitlements: {len(non_toc_parcels)}')\n",
    "print(f'# parcels with both TOC and non TOC entitlements: {len(have_both_parcels)}')\n",
    "print(f'double check sum: {len(toc_parcels) + len(non_toc_parcels) - len(have_both_parcels)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'% parcels with TOC entitlements: {len(toc_parcels) / len(m11)}')\n",
    "print(f'% parcels with non TOC entitlements: {len(non_toc_parcels) / len(m11)}')\n",
    "print(f'% parcels with both entitlements: {len(have_both_parcels) / len(m11)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m11.TOC_Tier.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toc_parcels.zone_class.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_toc_parcels.zone_class.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m12 = pd.merge(m11, parcels2, on = ['AIN', 'TOC_Tier'], how = 'inner').drop(columns = ['x', 'y', 'obs', 'num_obs'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m12.rename(columns = {'centroid':'geometry'}, inplace = True)\n",
    "m12 = gpd.GeoDataFrame(m12)\n",
    "m12.crs = {'init':'epsg:2229'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m12.to_file(driver = 'GeoJSON', filename = '../gis/intermediate/toc_eligible_parcels_with_entitlements.geojson')\n",
    "\n",
    "s3.upload_file('../gis/intermediate/toc_eligible_parcels_with_entitlements.geojson', \n",
    "               f'{bucket_name}', 'gis/intermediate/toc_eligible_parcels_with_entitlements.geojson')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Breakdown by TOC Tiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m13 = m12.groupby('TOC_Tier').agg({'AIN':'count', 'num_TOC':'sum', 'num_nonTOC':'sum'}).reset_index()\n",
    "\n",
    "for i in ['TOC', 'nonTOC']:\n",
    "    new_col = f'pct_{i}'\n",
    "    numerator = f'num_{i}'\n",
    "    m13[new_col] = m13[numerator] / (m13.num_TOC + m13.num_nonTOC)\n",
    "    \n",
    "m13['all_AIN'] = m13.AIN.sum()\n",
    "m13['pct_AIN'] = m13.AIN / m13.all_AIN\n",
    "\n",
    "m13"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Breakdown by Zone Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m14 = m12.groupby('zone_class').agg({'AIN': 'count', 'num_TOC': 'sum', 'num_nonTOC': 'sum'}).reset_index()\n",
    "\n",
    "for i in ['TOC', 'nonTOC']:\n",
    "    new_col = f'pct_{i}'\n",
    "    numerator = f'num_{i}'\n",
    "    m14[new_col] = m14[numerator] / (m14.num_TOC + m14.num_nonTOC)\n",
    "    \n",
    "m14['all_AIN'] = m14.AIN.sum()\n",
    "m14['pct_AIN'] = m14.AIN / m14.all_AIN\n",
    "\n",
    "m14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = pd.ExcelWriter('../outputs/toc_charts.xlsx', engine = 'xlsxwriter')\n",
    "\n",
    "m13.to_excel(writer, sheet_name = 'entitlements_by_tier')\n",
    "m14.to_excel(writer, sheet_name = 'entitlements_by_zone')\n",
    "\n",
    "writer.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
